{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3760f4",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "- Host OS: Ubuntu 20.04 lts\n",
    "- Using Docker Image 'mltooling/ml-workspace-gpu' (docker pull mltooling/ml-workspace-gpu)\n",
    "- Single Nvidia GPU (RTX 3080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d991b9",
   "metadata": {},
   "source": [
    "# Check computing resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456ee125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:17:35.564951Z",
     "start_time": "2022-11-11T04:17:35.372838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\r\n"
     ]
    }
   ],
   "source": [
    "#### The number of CPU cores\n",
    "!grep -c processor /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1e89be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:17:35.793660Z",
     "start_time": "2022-11-11T04:17:35.569949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 11 04:17:35 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 48%   51C    P8    38W / 370W |    236MiB / 12288MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "#### GPU information\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7101f6ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:17:40.568061Z",
     "start_time": "2022-11-11T04:17:40.197873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_count: 1\n",
      "device 0 capability (8, 6)\n",
      "device 0 name NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(\"device_count: {}\".format(device_count))\n",
    "    for device_num in range(device_count):\n",
    "        print(\"device {} capability {}\".format(\n",
    "            device_num,\n",
    "            torch.cuda.get_device_capability(device_num)))\n",
    "        print(\"device {} name {}\".format(\n",
    "            device_num, \n",
    "            torch.cuda.get_device_name(device_num)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"no cuda device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ff726",
   "metadata": {},
   "source": [
    "# 0. Customize Train Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4116d318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:17:42.431610Z",
     "start_time": "2022-11-11T04:17:42.426212Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cpus = 16\n",
    "num_gpus = 1\n",
    "seed = 1234\n",
    "model_name = \"xlm-roberta-base\" # bert-base-multilingual-cased ; klue/roberta-base ; bert-base-cased, etc.\n",
    "train_proportion = 0.7 # train set : eval set = 7 : 3\n",
    "do_hpo = False\n",
    "\n",
    "# If you want to search best hyperparameters using ray tune, parameters below should be set\n",
    "n_trials = 5\n",
    "std = 0.1\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c4c676",
   "metadata": {},
   "source": [
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1aeec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:17:55.825440Z",
     "start_time": "2022-11-11T04:17:55.820438Z"
    }
   },
   "outputs": [],
   "source": [
    "## Need to check if packages are compatible ##\n",
    "\n",
    "# !pip install accelerate nvidia-ml-py3\n",
    "# !pip install datasets==2.4.0\n",
    "# !pip install huggingface_hub==0.9.1\n",
    "# !pip install transformers==4.22.1 \n",
    "# !pip install pyarrow==9.0.0\n",
    "# !pip install -q ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50efac54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:17:56.620905Z",
     "start_time": "2022-11-11T04:17:55.987086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.22.1\n",
      "2.4.0\n",
      "0.9.1\n",
      "9.0.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import pyarrow\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)\n",
    "print(huggingface_hub.__version__)\n",
    "print(pyarrow.__version__)\n",
    "\n",
    "# 4.22.1\n",
    "# 2.4.0\n",
    "# 0.9.1\n",
    "# 9.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4db4071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:17:57.786060Z",
     "start_time": "2022-11-11T04:17:56.621717Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 'You can use tf32' if you are acessing Ampere hardware\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "from datasets import load_dataset, load_metric, ClassLabel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.examples.pbt_transformers.utils import (\n",
    "    download_data,\n",
    "    build_compute_metrics_fn,\n",
    ")\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from transformers import (\n",
    "    glue_tasks_num_labels,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    GlueDataset,\n",
    "    GlueDataTrainingArguments,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3f454",
   "metadata": {},
   "source": [
    "# 2. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305f41f",
   "metadata": {},
   "source": [
    "2 files are needed (`{data_name}_train.csv` and `{data_name}_test.csv`) in your data directory (in this case, `data_splited/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6e81be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:18:12.378356Z",
     "start_time": "2022-11-11T04:18:11.439656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-eb5690d969448b40\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7742c515edf4889b020d20bf547bc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'past_history', 'treatment_effect', 'examination', 'label'],\n",
       "        num_rows: 3756\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'past_history', 'treatment_effect', 'examination', 'label'],\n",
       "        num_rows: 940\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name = \"cardiovascular_sev_dataset\" \n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': f'../data_splited/{data_name}_train.csv',\n",
    "                                          'test': f'../data_splited/{data_name}_test.csv'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d565c1",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0fd01bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:18:13.230299Z",
     "start_time": "2022-11-11T04:18:13.216192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 64세 남환은 HTN, DM 의 과거력 (on medication) 있는 분으로 \n",
      "\n",
      "\n",
      "\n",
      "1주일 전부터 왼쪽 가슴쪽으로의 통증이 있어 OPD 내원하여 EKG \n",
      "\n",
      "\n",
      "\n",
      "상에서 Inf. Infarction 의심되고 2011.08.11 시행한 Echo 상에서 \n",
      "\n",
      "\n",
      "\n",
      "RCA territory RWMA 있었으며 EF 51%, conc. LVH, E/E' 12  asc. \n",
      "\n",
      "\n",
      "\n",
      "aorta 37mm  소견 있었으며 8월 12일 Angio 위해 입원하였으나 보\n",
      "\n",
      "\n",
      "\n",
      "호자 없어 시술하지 못하고 금일 CAG 시행위해 일일입원실 입원함\n"
     ]
    }
   ],
   "source": [
    "#### Select the column you want to tokenize and label column.\n",
    "\n",
    "dataset = dataset.remove_columns(['id', 'treatment_effect', 'examination'])\n",
    "dataset = dataset.rename_column(\"past_history\", \"text\")\n",
    "\n",
    "# dataset = dataset.remove_columns(['id', 'examination', 'past_history'])\n",
    "# dataset = dataset.rename_column(\"treatment_effect\", \"text\")\n",
    "\n",
    "# dataset = dataset.remove_columns(['id', 'treatment_effect', 'past_history'])\n",
    "# dataset = dataset.rename_column(\"examination\", \"text\")\n",
    "\n",
    "print(dataset['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f4d580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:18:16.112092Z",
     "start_time": "2022-11-11T04:18:16.084122Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-b38ea0e30a82d01f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-e00e6ae3dce802a8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-0d04eff8a8583e00.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-30e62e5b8f0cb9ea.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3756\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 940\n",
      "    })\n",
      "})\n",
      "본 64세 남환은 HTN DM 의 과거력 on medication 있는 분으로 1주일 전부터 왼쪽 가슴쪽으로의 통증이 있어 OPD 내원하여 EKG 상에서 Inf Infarction 의심되고 20110811 시행한 Echo 상에서 RCA territory RWMA 있었으며 EF 51 conc LVH EE 12  asc aorta 37mm  소견 있었으며 8월 12일 Angio 위해 입원하였으나 보호자 없어 시술하지 못하고 금일 CAG 시행위해 일일입원실 입원함\n"
     ]
    }
   ],
   "source": [
    "#### Remove NA rows\n",
    "\n",
    "dataset = dataset.filter(lambda row: pd.notnull(row[\"text\"]))\n",
    "\n",
    "#### Remove specal characters\n",
    "\n",
    "def remove_sp(example):\n",
    "    example[\"text\"]=re.sub(r'[^a-z|A-Z|0-9|ㄱ-ㅎ|ㅏ-ㅣ|가-힣| ]+', '', str(example[\"text\"]))\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(remove_sp)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69528e89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:18:36.735980Z",
     "start_time": "2022-11-11T04:18:31.846183Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-3aabaf0038fffc54.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-d41f9d74d055223b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3756\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 940\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Tokenizing \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation_side = 'left') # truncation_side = 'left' option remains last 512 tokens\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_batch = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True) # padding : ['longest', 'max_length', 'do_not_pad']\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cc8fc38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:18:36.742521Z",
     "start_time": "2022-11-11T04:18:36.736681Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-9372591042b600d7.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-eb5690d969448b40/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-9372591042b600d7.arrow\n"
     ]
    }
   ],
   "source": [
    "#### Train-Evalulation-Test Split \n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=seed).select(range(0,math.floor(len(tokenized_datasets[\"train\"])*train_proportion)))\n",
    "eval_dataset = tokenized_datasets[\"train\"].shuffle(seed=seed).select(range(math.floor(len(tokenized_datasets[\"train\"])*train_proportion), len(tokenized_datasets[\"train\"])))\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1e1ab11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T04:18:36.764483Z",
     "start_time": "2022-11-11T04:18:36.743219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([0.6868, 1.8385])\n"
     ]
    }
   ],
   "source": [
    "#### Applying class weights\n",
    "\n",
    "def class_weight(train_dataset) :\n",
    "    \n",
    "    train_labels = np.array(train_dataset[\"label\"])\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_labels), y = train_labels)\n",
    "    \n",
    "    weights = torch.tensor(class_weights, dtype = torch.float)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "weights = class_weight(train_dataset)\n",
    "print(f\"Class Weights: {weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf98414",
   "metadata": {},
   "source": [
    "# 4. Set model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4015ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:08:23.047965Z",
     "start_time": "2022-11-11T00:08:11.602834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 00:08:13,823\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "#### Initialize Ray\n",
    "ray.shutdown()\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, num_cpus=num_cpus, num_gpus=num_gpus, include_dashboard=False)\n",
    "\n",
    "####  Load the model \n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "        )\n",
    "\n",
    "#### Define metrics to use for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    metric1 = load_metric(\"accuracy\")\n",
    "    metric2 = load_metric(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric1.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = metric2.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1, \"objective\": accuracy+f1}\n",
    "\n",
    "#### batch size = 32, evaluate every 50 steps\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5, # config\n",
    "    weight_decay=0.1, # config\n",
    "    adam_beta1=0.1, # config\n",
    "    adam_beta2=0.1, # config\n",
    "    adam_epsilon=1.5e-06, # config\n",
    "    num_train_epochs=15, # config\n",
    "    max_steps=-1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,  # config\n",
    "    warmup_steps=0,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    no_cuda=num_gpus <= 0, \n",
    "    seed=seed,  # config\n",
    "    bf16=False, # Need torch>=1.10, Ampere GPU with cuda>=11.0\n",
    "    fp16=True,\n",
    "    tf32=True, \n",
    "    eval_steps = 50,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model=\"objective\", # f1 + acc\n",
    "    report_to=\"none\",\n",
    "    skip_memory_metrics=True,\n",
    "    gradient_checkpointing=True\n",
    "    )\n",
    "\n",
    "#### Customize trainer class to apply class weights\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        weight = weights.to(device)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "trainer = CustomTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "#### Fix batch_size in each trial\n",
    "tune_config = {\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"max_steps\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351304bf",
   "metadata": {},
   "source": [
    "# 4.5. Hyperparameter Optimization with PBT (Optional)\n",
    "\n",
    "- If you want to train model with fixed hyperparameters in `training_args`, skip this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c71acd2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:08:23.052881Z",
     "start_time": "2022-11-11T00:08:23.048772Z"
    }
   },
   "outputs": [],
   "source": [
    "if do_hpo == True:\n",
    "    \n",
    "    #### PBT schduler\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"objective\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=1,\n",
    "        hyperparam_mutations={\n",
    "            \"num_train_epochs\": tune.randint(2, 20),\n",
    "            \"seed\": tune.randint(1, 9999),\n",
    "            \"weight_decay\": tune.uniform(0.0, 0.3),\n",
    "            \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
    "            \"warmup_ratio\": tune.uniform(0.0, 0.3),\n",
    "            \"adam_beta1\": tune.loguniform(1e-2, 1),\n",
    "            \"adam_beta2\": tune.loguniform(1e-3, 1),\n",
    "            \"adam_epsilon\": tune.loguniform(1e-8, 1e-5),\n",
    "        }, \n",
    "    )\n",
    "\n",
    "    #### Define columns to report\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns={\n",
    "            \"weight_decay\": \"w_decay\",\n",
    "            \"learning_rate\": \"lr\",\n",
    "            \"per_device_train_batch_size\": \"train_bs/gpu\",\n",
    "            \"num_train_epochs\": \"num_epochs\",\n",
    "        },\n",
    "        metric_columns=[\"eval_f1\", \"eval_accuracy\", \"eval_objective\", \"eval_loss\", \"epoch\", \"training_iteration\"]\n",
    "    )\n",
    "\n",
    "    #### Early stopping\n",
    "    stopper = tune.stopper.ExperimentPlateauStopper(metric=\"objective\", \n",
    "                                                    std=std,\n",
    "                                                    top=n_trials,\n",
    "                                                    mode=\"max\",\n",
    "                                                    patience=patience\n",
    "                                                    )\n",
    "\n",
    "    #### HPO\n",
    "    hpo_result = trainer.hyperparameter_search(\n",
    "        hp_space = lambda _: tune_config,\n",
    "        direction = \"maximize\",\n",
    "        backend=\"ray\",\n",
    "        reuse_actors = True,\n",
    "        n_trials=n_trials,\n",
    "        resources_per_trial={\"cpu\": num_cpus, \"gpu\": num_gpus},\n",
    "        scheduler=scheduler,\n",
    "        keep_checkpoints_num=1,\n",
    "        checkpoint_score_attr=\"training_iteration\",\n",
    "        stop=stopper,\n",
    "        progress_reporter=reporter,\n",
    "        local_dir=\"./test-results\",\n",
    "        name=\"tune_transformer_pbt\",\n",
    "        log_to_file=True,\n",
    "    )\n",
    "    \n",
    "    print(hpo_result)\n",
    "    \n",
    "    for n, v in hpo_result.hyperparameters.items():\n",
    "        setattr(trainer.args, n, v)\n",
    "    \n",
    "    print(trainer.args)\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd926b3c",
   "metadata": {},
   "source": [
    "# 5. Train, evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54069146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:44.186716Z",
     "start_time": "2022-11-11T00:08:23.053514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2629\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1230\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1230/1230 17:17, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Objective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686573</td>\n",
       "      <td>0.729370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.546336</td>\n",
       "      <td>0.797693</td>\n",
       "      <td>0.606897</td>\n",
       "      <td>1.404590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.361667</td>\n",
       "      <td>0.848270</td>\n",
       "      <td>0.762829</td>\n",
       "      <td>1.611099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.354842</td>\n",
       "      <td>0.841171</td>\n",
       "      <td>0.757781</td>\n",
       "      <td>1.598952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.299786</td>\n",
       "      <td>0.898846</td>\n",
       "      <td>0.821875</td>\n",
       "      <td>1.720721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.294433</td>\n",
       "      <td>0.898846</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>1.721276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.293469</td>\n",
       "      <td>0.885537</td>\n",
       "      <td>0.810573</td>\n",
       "      <td>1.696110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.302301</td>\n",
       "      <td>0.914818</td>\n",
       "      <td>0.843648</td>\n",
       "      <td>1.758466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.257059</td>\n",
       "      <td>0.892635</td>\n",
       "      <td>0.822840</td>\n",
       "      <td>1.715476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.273280</td>\n",
       "      <td>0.920142</td>\n",
       "      <td>0.858044</td>\n",
       "      <td>1.778186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.469398</td>\n",
       "      <td>0.914818</td>\n",
       "      <td>0.827957</td>\n",
       "      <td>1.742775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.291704</td>\n",
       "      <td>0.922804</td>\n",
       "      <td>0.857610</td>\n",
       "      <td>1.780414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.299208</td>\n",
       "      <td>0.930790</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>1.806980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.333131</td>\n",
       "      <td>0.933452</td>\n",
       "      <td>0.876847</td>\n",
       "      <td>1.810299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.362466</td>\n",
       "      <td>0.917480</td>\n",
       "      <td>0.856703</td>\n",
       "      <td>1.774183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.344260</td>\n",
       "      <td>0.927240</td>\n",
       "      <td>0.864238</td>\n",
       "      <td>1.791479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.346259</td>\n",
       "      <td>0.920142</td>\n",
       "      <td>0.864048</td>\n",
       "      <td>1.784190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.337686</td>\n",
       "      <td>0.930790</td>\n",
       "      <td>0.876582</td>\n",
       "      <td>1.807372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.367499</td>\n",
       "      <td>0.932564</td>\n",
       "      <td>0.877419</td>\n",
       "      <td>1.809984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.432507</td>\n",
       "      <td>0.932564</td>\n",
       "      <td>0.875410</td>\n",
       "      <td>1.807974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.461301</td>\n",
       "      <td>0.935226</td>\n",
       "      <td>0.879736</td>\n",
       "      <td>1.814963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.477095</td>\n",
       "      <td>0.932564</td>\n",
       "      <td>0.874587</td>\n",
       "      <td>1.807152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.490005</td>\n",
       "      <td>0.931677</td>\n",
       "      <td>0.873147</td>\n",
       "      <td>1.804824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.498077</td>\n",
       "      <td>0.931677</td>\n",
       "      <td>0.873147</td>\n",
       "      <td>1.804824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint-1000 (score: 1.8079741661454318).\n"
     ]
    }
   ],
   "source": [
    "train_history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dc98b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:52.287855Z",
     "start_time": "2022-11-11T00:25:44.187625Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.43250736594200134,\n",
       " 'eval_accuracy': 0.9325643300798581,\n",
       " 'eval_f1': 0.8754098360655737,\n",
       " 'eval_objective': 1.8079741661454318,\n",
       " 'eval_runtime': 8.0962,\n",
       " 'eval_samples_per_second': 139.2,\n",
       " 'eval_steps_per_second': 17.416,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc9c16",
   "metadata": {},
   "source": [
    "# 6. Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4aecdc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:59.350932Z",
     "start_time": "2022-11-11T00:25:52.288585Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 940\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-3.338 ,  3.521 ],\n",
       "       [ 3.303 , -2.922 ],\n",
       "       [ 3.799 , -3.658 ],\n",
       "       ...,\n",
       "       [ 0.2651,  0.3213],\n",
       "       [-2.559 ,  2.799 ],\n",
       "       [-2.998 ,  2.998 ]], dtype=float16), label_ids=array([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]), metrics={'test_loss': 0.5089142322540283, 'test_accuracy': 0.924468085106383, 'test_f1': 0.8687615526802218, 'test_objective': 1.7932296377866048, 'test_runtime': 7.0574, 'test_samples_per_second': 133.194, 'test_steps_per_second': 16.72})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = trainer.predict(test_dataset=test_dataset)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ef40ce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:59.353643Z",
     "start_time": "2022-11-11T00:25:59.351596Z"
    }
   },
   "outputs": [],
   "source": [
    "label_test = list(pred.label_ids)\n",
    "pred_test = list(map(lambda x: x.index(max(x)), pred.predictions.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3999752c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:59.375039Z",
     "start_time": "2022-11-11T00:25:59.354258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[634  29]\n",
      " [ 42 235]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(label_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b87a4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:59.390418Z",
     "start_time": "2022-11-11T00:25:59.376844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924468085106383\n",
      "0.8687615526802218\n",
      "0.8483754512635379\n",
      "0.8901515151515151\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(label_test, pred_test)\n",
    "f1 = f1_score(label_test, pred_test)\n",
    "recall = recall_score(label_test, pred_test)\n",
    "precision = precision_score(label_test, pred_test)\n",
    "\n",
    "print(accuracy)\n",
    "print(f1)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a358c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:59.403893Z",
     "start_time": "2022-11-11T00:25:59.391886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>상기 46세 남환  HTNnot med 외 특이 과거력 없는 분으로  내원 2시간 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>상기 45세 남환 혈압 높다는 얘기 들었으나 medication하지 않았다고 하며 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>상기 74세 남환 CAD 3VD  LM sp midCAB for LAD 202004...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>상기 55세 남환 HTN DM type 2 Dyslipidemia ESRD on P...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>상기 남환 특이 과거력 없던 분으로 내원 3시간 50분전5AM경부터 chest pa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>본 환자는 특이내과적 병력 없던 분으로 1년전과 3개월전 각각 약 30분 지속되는 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>상기 52세 남환 2005 HTN DM cerebral infarction Hx있는...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>상기 57세 남환 HTN2yr DM1mo 과거력 있으며 05pack x 35yr c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>상기 78세 남환 고혈압no current med 당뇨on PO med 과거력 있는...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>상기 57세 남환 acute MI CAOD 3vd PTCA c stent inser...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>940 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  pred\n",
       "0    상기 46세 남환  HTNnot med 외 특이 과거력 없는 분으로  내원 2시간 ...      1     1\n",
       "1    상기 45세 남환 혈압 높다는 얘기 들었으나 medication하지 않았다고 하며 ...      0     0\n",
       "2    상기 74세 남환 CAD 3VD  LM sp midCAB for LAD 202004...      0     0\n",
       "3    상기 55세 남환 HTN DM type 2 Dyslipidemia ESRD on P...      0     0\n",
       "4    상기 남환 특이 과거력 없던 분으로 내원 3시간 50분전5AM경부터 chest pa...      1     1\n",
       "..                                                 ...    ...   ...\n",
       "935  본 환자는 특이내과적 병력 없던 분으로 1년전과 3개월전 각각 약 30분 지속되는 ...      0     0\n",
       "936  상기 52세 남환 2005 HTN DM cerebral infarction Hx있는...      0     0\n",
       "937  상기 57세 남환 HTN2yr DM1mo 과거력 있으며 05pack x 35yr c...      0     1\n",
       "938  상기 78세 남환 고혈압no current med 당뇨on PO med 과거력 있는...      1     1\n",
       "939  상기 57세 남환 acute MI CAOD 3vd PTCA c stent inser...      1     1\n",
       "\n",
       "[940 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.concat([pd.DataFrame(test_dataset['text'], columns=['text']), \n",
    "                          pd.DataFrame(label_test, columns=['label']),\n",
    "                          pd.DataFrame(pred_test, columns=['pred'])],\n",
    "                         axis=1\n",
    "                        )\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba43d397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:59.415575Z",
     "start_time": "2022-11-11T00:25:59.404697Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_results.to_csv('./ph_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343ed63",
   "metadata": {},
   "source": [
    "# 7. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdd9d684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:59.425911Z",
     "start_time": "2022-11-11T00:25:59.416187Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_path = f\"sev_exam_1.0\"\n",
    "# trainer.model.save_pretrained(model_path)\n",
    "# tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7adc42ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T00:25:59.436279Z",
     "start_time": "2022-11-11T00:25:59.426661Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model / pred\n",
    "\n",
    "# load_model = AutoModelForSequenceClassification.from_pretrained(\"sev_exam_1.0/\")\n",
    "# load_tokenizer = AutoTokenizer.from_pretrained(\"sev_exam_1.0/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3f419",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "https://bo-10000.tistory.com/154  \n",
    "https://huggingface.co/blog/ray-tune  \n",
    "https://docs.ray.io/en/latest/tune/examples/pbt_transformers.html  \n",
    "https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/#schedulers-vs-search-algorithms  \n",
    "https://docs.ray.io/en/latest/tune/api_docs/search_space.html  \n",
    "https://docs.ray.io/en/latest/tune/tutorials/tune-advanced-tutorial.html  \n",
    "https://keras.io/examples/keras_recipes/sample_size_estimate/  \n",
    "https://www.topbots.com/fine-tune-transformers-in-pytorch/  \n",
    "https://docs.ray.io/en/latest/tune/api_docs/schedulers.html  \n",
    "https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/  \n",
    "https://docs.ray.io/en/latest/tune/faq.html  \n",
    "https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#population-based-training-tune-schedulers-populationbasedtraining  \n",
    "https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.hyperparameter_search  \n",
    "https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#optuna-tune-search-optuna-optunasearch  \n",
    "https://kyunghyunlim.github.io/nlp/ml_ai/2021/09/22/hugging_face_5.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1aff3f",
   "metadata": {},
   "source": [
    "# Future Works\n",
    " - step이 늘어나면서 성능이 어떻게 좋아지는지, hp조합에 따라 어떻게 좋아지는지 시각화 추가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
